import math as m # temporary fix to undefined variable "m" error
from random import seed
from random import random
from math import exp

# Code reference: https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/

# ==== Initialize Neural Network ====
def start_network(main_input: int, hidden_neur: int, output_neur: int):

    """
        This function initializes the neural network to be trained

            parameter (main_input): int
                Specifies number of inputs to feed the network
            
            parameter (hidden_neur): int
                Specifies the number of neurons in the hidden layer

            parameter (output_neur): int
                Specifies the number of neurons in the output layer
    """

    # network list
    network = []

    # PLEASE NOTE: +1 is added to include the bias to the neuron
    """
        Below generates the hidden layer(s)
        > It is created by randomly generating the weights and biases for the input channels for
            the number of neurons in the hidden layer
    """
    hidden_layer = [{'weights':[random() for i in range(main_input + 1)]} for i in range(hidden_neur)]
    network.append(hidden_layer)

    """
        Below generates the output layer(s)
        > It is created by randomly generating the weights and biases for the channels fed to the 
            number of neurons in the output layer
    """
    output_layer = [{'weights':[random() for i in range(hidden_neur + 1)]} for i in range(output_neur)]
    network.append(output_layer)

    return network # return the layer information

# ==== Forward Propagation ====
def activate_neur(weights, inputs):
    """
        This function generates the activation value that a neuron will have which will determine
        the neurons output. The value determines if whether or not it is going to activate it

        The higher the number, the greater the activation

        It replicates that of a linear regression function
        > neuron activation value = (neuron weight * input value) + bias

        parameter (weights): float
            Specifies the weight of each neuron from the "start_network" function
        
        parameter (inputs): float
            Specifies the value from the input to be fed to a neuron
    """
    activation = weights[-1] # assumes the generated BIAS is going to be at the end of the list of weights
    for i in range(len(weights)-1): # adds the bias to the product of the weight and input value
        activation += weights[i] * inputs[i]
    return activation

def transfer_func(activation):
    """
        This function generates the transfer function to transfer the output of the neuron to the next
        in the proceeding layer

        The activation function being used here is Sigmoid

        paremeter (activation): float
            Attains the activation values from the "activate_neur" function
    """
    sigmoid_func = 1 / 1 + exp(-activation)
    return sigmoid_func 

def forward_propagation(network, row):
    """
        This function propagates signals through the neural network layers to arrive at an output
        
        parameter (network): list
            Attains the neural network
        
        parameter (row): list
            Attains a row of data from a data set
    """
    get_inputs = row
    for layer in network: # loops through each layer generated by the network
        new_inputs = [] # makes a list of inputs for each layer looped
        for neuron in layer: # loops through each neuron in the layer
            activation = activate_neur(neuron['weights'], get_inputs)
            neuron['output'] = transfer_func(activation) # the calculated output from the transfer function will be the output of the neuron
            new_inputs.append(neuron['output']) # add the data to the new input list
        inputs = new_inputs # changes from initial inputs to the new ones
    return inputs

# ==== Back Propagation ====
def transfer_derivative(output):
    """
        This function gets the transfer derivative of the sigmoid function. The derivative of any continuous function
        is a slope. The value returned is the slope of the output value of the neuron        
        The slope represents a "gradient descent method". To have the best possible output is to have the gradient descent 
        line as flat as possible along the transfer function.
        
        Based on the slope, we can determine the error between the desired and the actual output

        (I'M NOT SURE IF THIS IS RIGHT OR NOT, I THINK IT IS, I'M NOT ENTIRELY SURE TBH)

        Links:
            (an image)  https://www.google.com/url?sa=i&url=https%3A%2F%2Fblog.clairvoyantsoft.com%2Fthe-ascent-of-gradient-descent-23356390836f&psig=AOvVaw1My1CfaW5TBOZGz3rQnYpj&ust=1629650051320000&source=images&cd=vfe&ved=0CAsQjRxqFwoTCOCN2djGwvICFQAAAAAdAAAAABAD
            (thread explanation) https://intellipaat.com/community/17266/why-do-we-take-the-derivative-of-the-transfer-function-in-calculating-back-propagation-algorithm

        parameter (output):
            Takes the output from the output layer of the NN
    """
    t_derivative = output * (1 - output) # based on the Chain Rule
    return t_derivative


def backpropagate_error(network, desired):
    """
        This function is in charge of backpropagating the error through the network in order to see the differences in outputs between each neuron. These differences ultimately decide
        the accuracy of the model once it has been fully trained
        
        It is broken up by iterating through each layer and assigning the error differences to each neuron in them
        
        It has 2 different conditions to when assigning the error. The first being the hidden layers, the second being the output layer.
        
        parameter (network):
            Takes the generated network
         
        parameter (desired):
            Represents the desired output (the actual output we want from the network)
    """
    for layer_index in reversed(range(len(network))): # loop through each layer in the network in reverse
        layer = network[layer_index]
        err_list = []
        if layer_index != len(network)-1: # if the current layer is NOT the output layer
            for neuron_index in range(len(layer)):
                calc_error = 0.0
                for neuron in network[layer_index + 1]: # loop through each neuron in the next layer after the current one 
                    calc_error += neuron['weights'][neuron_index] * neuron['delta'] # calculate the error of the hidden layer neuron output
                    # above calculation is based on the weight of the neuron * the difference between expected output and actual (RE-VISIT)
                err_list.append(calc_error)
        else:
            for neuron_index in range(len(layer)): # loop through each neuron in the output layer and append the error
                neuron = layer[neuron_index]
                out_error = 0.0
                out_error += desired[neuron_index] - neuron['output']
                err_list.append(out_error)
        for neuron_index in range(len(layer)):
            neuron = layer[neuron_index]
            neuron['delta'] = err_list[neuron_index] * transfer_derivative(neuron['output']) # error difference 


# ==== POST-ERROR ANALYSIS ====
# ==== Weight Updates ====
def update_weights(network, data_row, learn_rate):
    """
        This function updates the weights of each of the neurons based on the error of each neuron and a desired learning rate

        The learning rate is a varying percentage which decides the speed of the learning process and accuracy of the NN. If the learning rate is set to a low
        value, then training will take much longer than if the rate were set to a higher number. Although having a slower learning rate can greatly increase the
        NN's ability to predict. 
        The set rate (in %) is the value that the weights will update by
            neuron['delta'] = err_list[neuron_index] * transfer_derivative(neuron['output']) # error difference 

        parameter (network):
            Takes the generated network
        
        parameter (row):
            Rows from the dataset as an index (I think)
        
        parameter (learning rate):
            The percent that the weights are changed by
    """
    for layer_index in range(len(network)):
        inputs = data_row[:-1]
        if layer_index != 0:
            inputs = [neuron['output'] for neuron in network[layer_index-1]]
        for neuron in network[layer_index]:
            for neuron_index in range(len(inputs)):
                neuron['weights'][neuron_index] += learn_rate * neuron['delta'] * inputs[neuron_index] # hidden layer weights
            neuron['weights'][-1] += learn_rate * neuron['delta'] # output layer weights

# ==== Training the Network ====
def train(network, dataset, learn_rate, epochs, n_outputs):
    for epoch_index in range(epochs):
        progression = 0
        for row in dataset:
            outputs = forward_propagation(network, row)
            expected = [0 for error_index in range(n_outputs)] # hot encoding, look into this
            expected[row[-1]] = 1 # hot encoding uses binary assigning
            progression += sum([(expected[error_index] - outputs[error_index]) ** 2 for error_index in range(len(expected))])

            backpropagate_error(network, expected)
            update_weights(network, row, learn_rate)
        print(f'epoch = {epoch_index}, learning = {learn_rate}, error = {progression}')


def predict(network, row):
    outputs = forward_propagation(network, row)
    return outputs.index(max(outputs))

# !!!! TESTING !!!!
data = [[2.7810836,2.550537003,0],
	[1.465489372,2.362125076,0],
	[3.396561688,4.400293529,0],
	[1.38807019,1.850220317,0],
	[3.06407232,3.005305973,0],
	[7.627531214,2.759262235,1],
	[5.332441248,2.088626775,1],
	[6.922596716,1.77106367,1],
	[8.675418651,-0.242068655,1],
	[7.673756466,3.508563011,1]]

network = [[{'weights': [-1.482313569067226, 1.8308790073202204, 1.078381922048799]}, {'weights': [0.23244990332399884, 0.3621998343835864, 0.40289821191094327]}],
	[{'weights': [2.5001872433501404, 0.7887233511355132, -1.1026649757805829]}, {'weights': [-2.429350576245497, 0.8357651039198697, 1.0699217181280656]}]]

for row in data:
    prediction = predict(network, row)
    print(f'Expected: {row[-1]}, Got: {prediction}')